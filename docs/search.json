[
  {
    "objectID": "HW9_Modeling_Practice.html",
    "href": "HW9_Modeling_Practice.html",
    "title": "HW9 Modeling Practice",
    "section": "",
    "text": "library(baguette)\n\nWarning: package 'baguette' was built under R version 4.5.2\n\n\nLoading required package: parsnip\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.5.1\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.5.1\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(recipes)\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nLoading required package: dplyr\n\n\nWarning: package 'dplyr' was built under R version 4.5.1\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.5.1\n\n\nWarning: package 'tidyr' was built under R version 4.5.1\n\n\nWarning: package 'purrr' was built under R version 4.5.1\n\n\nWarning: package 'stringr' was built under R version 4.5.1\n\n\nWarning: package 'forcats' was built under R version 4.5.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.2     ✔ tibble  3.3.0\n✔ purrr   1.1.0     ✔ tidyr   1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ stringr::fixed() masks recipes::fixed()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ tailor       0.1.0 \n✔ dials        1.4.2      ✔ tune         2.0.1 \n✔ infer        1.0.9      ✔ workflows    1.3.0 \n✔ modeldata    1.5.1      ✔ workflowsets 1.1.1 \n✔ rsample      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'broom' was built under R version 4.5.1\n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ stringr::fixed()  masks recipes::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(tree) \n\nWarning: package 'tree' was built under R version 4.5.2\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.5.1\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nraw_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      locale = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(forcats)\n\nbike_data &lt;- \n  raw_data |&gt;\n  rename(\n    DATE = `Date`,\n    RBC = `Rented Bike Count`,\n    HOUR = `Hour`,\n    \"TEMP(deg C)\" = `Temperature(°C)`,\n    \"RH(%)\" = `Humidity(%)`,\n    \"WD_SPD(m/s)\" = `Wind speed (m/s)`,\n    \"VIS(10m)\" = `Visibility (10m)`,\n    \"DP_TEMP(deg C)\" = `Dew point temperature(°C)`,\n    \"SOL_RAD(MJ/m2)\" = `Solar Radiation (MJ/m2)`,\n    \"RAIN_FALL(mm)\" = `Rainfall(mm)`,\n    \"SNOW_FALL(cm)\" = `Snowfall (cm)`,\n    SEASONS = `Seasons`,\n    HOLIDAY = `Holiday`,\n    FUNC_DAY = `Functioning Day`\n  ) |&gt;\n  mutate(\n    DATE = dmy(DATE),\n    HOLIDAY = as_factor(HOLIDAY),\n    SEASONS = as_factor(SEASONS),\n    FUNC_DAY = as_factor(FUNC_DAY)\n  )\n\nwrite.csv(bike_data, \"bike_data.csv\")\n\nSummarize Across Hours\n\nbd_sub &lt;- bike_data |&gt; \n  group_by(DATE, SEASONS, HOLIDAY) |&gt;\n  summarise(across(where(is.numeric) , mean, na.rm = TRUE, .names = '{col}_MEAN'),\n            SUM_RBC = sum(RBC),\n            \"SUM_RAIN_FALL(mm)\" = sum(`RAIN_FALL(mm)`),\n            \"SUM_SNOW_FALL(cm)\" = sum(`SNOW_FALL(cm)`),) |&gt;\n            select(DATE, SEASONS, HOLIDAY, starts_with(\"SUM\"), everything(), -RBC_MEAN, -HOUR_MEAN, -`RAIN_FALL(mm)_MEAN`, -`SNOW_FALL(cm)_MEAN`)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(where(is.numeric), mean, na.rm = TRUE, .names =\n  \"{col}_MEAN\")`.\nℹ In group 1: `DATE = 2017-12-01`, `SEASONS = Winter`, `HOLIDAY = No Holiday`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'DATE', 'SEASONS'. You can override using\nthe `.groups` argument.\n\ncolnames(bd_sub) &lt;- c(\"DATE\", \"SEASONS\", \"HOLIDAY\", \"SUM_RBC\", \"SUM_RAIN_FALL\", \"SUM_SNOW_FALL\", \"TEMP\", \"RH\", \"WD_SPD\", \"VIS\", \"DP_TEMP\", \"SOL_RAD\")\n                        \nwrite.csv(bd_sub, \"bd_sub_after.csv\")"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#read-data-and-library-setup",
    "href": "HW9_Modeling_Practice.html#read-data-and-library-setup",
    "title": "HW9 Modeling Practice",
    "section": "",
    "text": "library(baguette)\n\nWarning: package 'baguette' was built under R version 4.5.2\n\n\nLoading required package: parsnip\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.5.1\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.5.1\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(recipes)\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nLoading required package: dplyr\n\n\nWarning: package 'dplyr' was built under R version 4.5.1\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.5.1\n\n\nWarning: package 'tidyr' was built under R version 4.5.1\n\n\nWarning: package 'purrr' was built under R version 4.5.1\n\n\nWarning: package 'stringr' was built under R version 4.5.1\n\n\nWarning: package 'forcats' was built under R version 4.5.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.2     ✔ tibble  3.3.0\n✔ purrr   1.1.0     ✔ tidyr   1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ stringr::fixed() masks recipes::fixed()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ tailor       0.1.0 \n✔ dials        1.4.2      ✔ tune         2.0.1 \n✔ infer        1.0.9      ✔ workflows    1.3.0 \n✔ modeldata    1.5.1      ✔ workflowsets 1.1.1 \n✔ rsample      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'broom' was built under R version 4.5.1\n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ stringr::fixed()  masks recipes::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(tree) \n\nWarning: package 'tree' was built under R version 4.5.2\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.5.1\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nraw_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      locale = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(forcats)\n\nbike_data &lt;- \n  raw_data |&gt;\n  rename(\n    DATE = `Date`,\n    RBC = `Rented Bike Count`,\n    HOUR = `Hour`,\n    \"TEMP(deg C)\" = `Temperature(°C)`,\n    \"RH(%)\" = `Humidity(%)`,\n    \"WD_SPD(m/s)\" = `Wind speed (m/s)`,\n    \"VIS(10m)\" = `Visibility (10m)`,\n    \"DP_TEMP(deg C)\" = `Dew point temperature(°C)`,\n    \"SOL_RAD(MJ/m2)\" = `Solar Radiation (MJ/m2)`,\n    \"RAIN_FALL(mm)\" = `Rainfall(mm)`,\n    \"SNOW_FALL(cm)\" = `Snowfall (cm)`,\n    SEASONS = `Seasons`,\n    HOLIDAY = `Holiday`,\n    FUNC_DAY = `Functioning Day`\n  ) |&gt;\n  mutate(\n    DATE = dmy(DATE),\n    HOLIDAY = as_factor(HOLIDAY),\n    SEASONS = as_factor(SEASONS),\n    FUNC_DAY = as_factor(FUNC_DAY)\n  )\n\nwrite.csv(bike_data, \"bike_data.csv\")\n\nSummarize Across Hours\n\nbd_sub &lt;- bike_data |&gt; \n  group_by(DATE, SEASONS, HOLIDAY) |&gt;\n  summarise(across(where(is.numeric) , mean, na.rm = TRUE, .names = '{col}_MEAN'),\n            SUM_RBC = sum(RBC),\n            \"SUM_RAIN_FALL(mm)\" = sum(`RAIN_FALL(mm)`),\n            \"SUM_SNOW_FALL(cm)\" = sum(`SNOW_FALL(cm)`),) |&gt;\n            select(DATE, SEASONS, HOLIDAY, starts_with(\"SUM\"), everything(), -RBC_MEAN, -HOUR_MEAN, -`RAIN_FALL(mm)_MEAN`, -`SNOW_FALL(cm)_MEAN`)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(where(is.numeric), mean, na.rm = TRUE, .names =\n  \"{col}_MEAN\")`.\nℹ In group 1: `DATE = 2017-12-01`, `SEASONS = Winter`, `HOLIDAY = No Holiday`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'DATE', 'SEASONS'. You can override using\nthe `.groups` argument.\n\ncolnames(bd_sub) &lt;- c(\"DATE\", \"SEASONS\", \"HOLIDAY\", \"SUM_RBC\", \"SUM_RAIN_FALL\", \"SUM_SNOW_FALL\", \"TEMP\", \"RH\", \"WD_SPD\", \"VIS\", \"DP_TEMP\", \"SOL_RAD\")\n                        \nwrite.csv(bd_sub, \"bd_sub_after.csv\")"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#split-the-data",
    "href": "HW9_Modeling_Practice.html#split-the-data",
    "title": "HW9 Modeling Practice",
    "section": "Split the Data",
    "text": "Split the Data\nUse functions from tidymodels to split the data into a training and test set (75/25 split). Use the strata argument to stratify the split on the seasons variable. • On the training set, create a 10 fold CV split.\nThe code splits the dataset bd_sub into training and test sets using tidymodels in R. initial_split(bd_sub, prop = 3/4, strata = SEASONS): Splits bd_sub so that 75% goes to training and 25% to testing, while preserving the distribution of SEASONS.\nbike_split: This is a split object created by initial_split(). It contains information about how the data was divided into training and testing sets, but does not contain the actual data frames.\nbike_train: This is the training set, extracted from bike_split using training(bike_split). It contains 70% of the original data, selected randomly.\nbike_test: This is the testing set, extracted from bike_split using testing(bike_split). It contains the remaining 30% of the data.\n\nlibrary(tidymodels)\n\nset.seed(222)\n# Put 3/4 of the data into the training set.  init_split is the Test Set. \nbike_data_split &lt;- initial_split(bd_sub, prop = 3/4, strata = SEASONS)\n               \n# Create data frames for the two sets:\nbike_train_data &lt;- training(bike_data_split)\nbike_test_data  &lt;- testing(bike_data_split)\n\n#On the training set, create a 10 fold CV split\nbike_CV_folds &lt;- vfold_cv(bike_train_data, 10)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#recipes-for-hw-9",
    "href": "HW9_Modeling_Practice.html#recipes-for-hw-9",
    "title": "HW9 Modeling Practice",
    "section": "Recipes for HW 9",
    "text": "Recipes for HW 9\nThis recipe is for all model types except MLR.\n\n  #Define Recipe 1\n  recipe1 &lt;- \n    recipe(SUM_RBC ~ ., data = bike_train_data) |&gt; \n    step_date(DATE, features = c(\"dow\")) |&gt;              \n    step_mutate(\n      DAY_TYPE = factor(\n        ifelse(\n          DATE_dow == \"Sat\" | DATE_dow == \"Sun\", \"WKEND\", \"WKDAY\"))) |&gt;\n    step_rm(DATE_dow, DATE) |&gt;\n    step_normalize(all_numeric_predictors()) |&gt;\n    step_dummy(all_nominal_predictors())\n\n\n# recipe1_prep &lt;- prep(recipe1, training = bike_train_data)\n# recipe1_baked &lt;- bake(recipe1_prep, new_data = bike_test_data)  \n#  \n# write.csv(recipe1_baked, \"recipe1_baked.csv\")\n\nMLR Recipe\n\n#Define Recipe 2\n\nrecipe2 &lt;- \n  recipe(SUM_RBC ~ ., data = bike_train_data) |&gt; \n  step_date(DATE, features = c(\"dow\")) |&gt;              \n  step_mutate(\n    DAY_TYPE = factor(\n      ifelse(\n        DATE_dow == \"Sat\" | DATE_dow == \"Sun\", \"WKEND\", \"WKDAY\"))) |&gt;\n  step_rm(DATE_dow, DATE) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact(\n  terms = ~ starts_with(\"HOLIDAY\"):starts_with(\"SEASONS\") +\n           `TEMP`:starts_with(\"SEASONS\") +\n           `TEMP`:`SUM_RAIN_FALL`\n)\n  \n# recipe2_prep &lt;- prep(recipe2, training = bike_train_data)\n# recipe2_baked &lt;- bake(recipe2_prep, new_data = bike_test_data)  \n#  \n# write.csv(recipe2_baked, \"recipe2_baked.csv\")"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#mlr-model-from-hw-8",
    "href": "HW9_Modeling_Practice.html#mlr-model-from-hw-8",
    "title": "HW9 Modeling Practice",
    "section": "MLR Model from HW 8",
    "text": "MLR Model from HW 8\nThe best model was already selected from HW 8 as Recipe 2, so that step is not shown here.\n\nDefine MLR Model Object\n\nMLR_model &lt;- linear_reg() |&gt;\nset_engine(\"lm\")\n\n\n\nDefine MLR Workflow\n\nMLR_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe2) |&gt;\n  add_model(MLR_model)\n\n\n\nFit the MLR to CV Folds\n\nMLR_CV_fit &lt;- MLR_wkf |&gt;\n  fit_resamples(bike_CV_folds)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\nI got this Warning in HW 8 and was unable to resolve it. Professor Post said the code looked fine.\n\n\nFit the MLR to the Training Set\n\nMLR_fitT &lt;- MLR_wkf |&gt;\nfit(bike_train_data)\n\nMLR_fit: This fits the chosen model (e.g., best multiple linear regression) to the entire training set.\n\n\nFit the MLR to the Test Set\n\nMLR_final &lt;- MLR_wkf |&gt;\n   last_fit(bike_data_split)\n\nMLR_final: This uses last_fit() to train the model on the training set and evaluate its predictions on the test set, giving performance metrics for out-of-sample data."
  },
  {
    "objectID": "HW9_Modeling_Practice.html#lasso-model",
    "href": "HW9_Modeling_Practice.html#lasso-model",
    "title": "HW9 Modeling Practice",
    "section": "LASSO Model",
    "text": "LASSO Model\n\nDefine LASSO Model Object\n\nLASSO_model &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\n\nCreate Workflow for LASSO Model\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe1) |&gt;\n  add_model(LASSO_model)\n\n\n\nFit the LASSO Workflow to CV Folds with tune_grid() and grid_regular()\nWe saw how to fit a workflow to a set of CV folds with fit_resample(). Since we have a tuning parameter here, we don’t want to use that function. Instead, we use tune_grid(). This function allows us to fit the model to CV folds but specify the set of tuning parameters to consider.\nThis implies we are actually doing a bunch of model fits on the CV folds (one for each tuning parameter). In the tune_grid() function we can specify the values of the tuning parameter with the grid = argument. grid_regular() is a function that can be used to choose a grid of reasonable values.\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_CV_folds,\n            grid = grid_regular(penalty(), levels = 200)) \n\n\n\nSelect Best Model with select_best()\n\nLASSO_best &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\n\n\nFit Best LASSO Model to the Training Set with finalize_workflow()\n\nLASSO_fitT &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_best) |&gt;\n  fit(bike_train_data)\n\nfinalize_workflow() tells R to finish our training with a specific setting of the terms we set to tune() in our model definition.\n\n\nFit the MLR to the Test Set\n\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_best) |&gt;\n  last_fit(bike_data_split)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#regression-tree-model",
    "href": "HW9_Modeling_Practice.html#regression-tree-model",
    "title": "HW9 Modeling Practice",
    "section": "Regression Tree Model",
    "text": "Regression Tree Model\n\nDefine Regression Tree (RTREE) Model and Engine\n\nRTREE_model &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\nCreate Workflow for RTREE Model\n\nRTREE_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe1) |&gt;\n  add_model(RTREE_model)\n\n\n\nFit the RTREE Model to CV Folds\n\nRTREE_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\nRTREE_fits &lt;- RTREE_wkf |&gt; \n  tune_grid(resamples = bike_CV_folds,\n            grid = RTREE_grid)\n\n\n\nSelect the Best RTREE Model with select_best()\n\nRTREE_best &lt;- select_best(RTREE_fits, metric = \"rmse\")\n\n\n\nFit Best RTREE model on the Training Set with finalize_workflow()\n\nRTREE_fitT &lt;- RTREE_wkf |&gt;\n  finalize_workflow(RTREE_best)|&gt;\n  fit(bike_train_data)\n\n\n\nFit the RTREE model to the Test Set\n\nRTREE_final &lt;- RTREE_wkf |&gt;\n  finalize_workflow(RTREE_best) |&gt;\n  last_fit(bike_data_split)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#bagged-tree-model",
    "href": "HW9_Modeling_Practice.html#bagged-tree-model",
    "title": "HW9 Modeling Practice",
    "section": "Bagged Tree Model",
    "text": "Bagged Tree Model\n\nDefine Bagged Tree (BTREE) Model and Engine\n\nBTREE_model &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\nset_engine(\"rpart\") |&gt;\nset_mode(\"regression\") |&gt;\n  translate()\n\n\n\nCreate Workflow for BTREE Model\n\nBTREE_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe1) |&gt;\nadd_model(BTREE_model)\n\n\n\nFit the BTREE Model to CV Folds\n\nBTREE_tuned &lt;- BTREE_wkf |&gt;\ntune_grid(resamples = bike_CV_folds,\ngrid = grid_regular(cost_complexity(),\nlevels = 15),\nmetrics = metric_set(rmse))\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\n\n→ A | warning: There was 1 warning in `dplyr::mutate()`.\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! package 'future' was built under R version 4.5.2\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\n\nSelect the Best BTREE Model with select_best()\n\nBTREE_best &lt;- select_best(BTREE_tuned, metric = \"rmse\")\n\n\n\nFit Best BTREE model on the Training Set with finalize_workflow()\n\nBTREE_fitT &lt;- BTREE_wkf |&gt;\n  finalize_workflow(BTREE_best)|&gt;\n  fit(bike_train_data)\n\n\n\nFit the BTREE model to the Test Set\n\nBTREE_final &lt;- BTREE_wkf |&gt;\n  finalize_workflow(BTREE_best) |&gt;\n  last_fit(bike_data_split)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#random-forest-model",
    "href": "HW9_Modeling_Practice.html#random-forest-model",
    "title": "HW9 Modeling Practice",
    "section": "Random Forest Model",
    "text": "Random Forest Model\n\nDefine Random Forest (RF) Model and Engine\n\nRF_model &lt;- rand_forest(mode = \"regression\", mtry = tune()) |&gt;\nset_engine(\"ranger\", importance = \"impurity\") \n\n\n\nCreate Workflow for RF Model\n\nRF_wkf &lt;- workflow() |&gt; \n  add_recipe(recipe1) |&gt;\nadd_model(RF_model)\n\n\n\nFit the RF Model to CV Folds\n\nRF_tuned &lt;- RF_wkf |&gt;\ntune_grid(resamples = bike_CV_folds,\ngrid = 7, \nmetrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n\n\n\nSelect the Best RF Model with select_best()\n\nRF_best &lt;- select_best(RF_tuned, metric = \"rmse\")\n\n\n\nFit Best RF model on the Training Set with finalize_workflow()\n\nRF_fitT &lt;- RF_wkf |&gt;\n  finalize_workflow(RF_best)|&gt;\n  fit(bike_train_data)\n\n\n\nFit the RF model to the Test Set\n\nRF_final &lt;- RF_wkf |&gt;\n  finalize_workflow(RF_best) |&gt;\n  last_fit(bike_data_split)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#compare-all-final-models-on-the-test-set",
    "href": "HW9_Modeling_Practice.html#compare-all-final-models-on-the-test-set",
    "title": "HW9 Modeling Practice",
    "section": "Compare All Final Models on the Test set",
    "text": "Compare All Final Models on the Test set\n\nrbind(MLR_final |&gt; compute_metrics(metric_set(rmse, mae)),\n      LASSO_final |&gt; compute_metrics(metric_set(rmse, mae)),\n      RTREE_final |&gt; compute_metrics(metric_set(rmse, mae)),\n      BTREE_final |&gt; compute_metrics(metric_set(rmse, mae)),\n      RF_final |&gt; compute_metrics(metric_set(rmse, mae)))\n\n# A tibble: 10 × 4\n   .metric .estimator .estimate .config        \n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n 1 rmse    standard       4954. pre0_mod0_post0\n 2 mae     standard       3292. pre0_mod0_post0\n 3 rmse    standard       5661. pre0_mod0_post0\n 4 mae     standard       4374. pre0_mod0_post0\n 5 rmse    standard       5266. pre0_mod0_post0\n 6 mae     standard       3744. pre0_mod0_post0\n 7 rmse    standard       5600. pre0_mod0_post0\n 8 mae     standard       3944. pre0_mod0_post0\n 9 rmse    standard       4661. pre0_mod0_post0\n10 mae     standard       3330. pre0_mod0_post0\n\n\nHere we can see that the RF Model is the best model with the lowest RMSE and MAE."
  },
  {
    "objectID": "HW9_Modeling_Practice.html#extract-the-final-model-fits-and-report-a-summary-of-each-model",
    "href": "HW9_Modeling_Practice.html#extract-the-final-model-fits-and-report-a-summary-of-each-model",
    "title": "HW9 Modeling Practice",
    "section": "Extract the Final Model Fits and Report a Summary of Each Model",
    "text": "Extract the Final Model Fits and Report a Summary of Each Model\n– For the LASSO and MLR models, report the final coefficient tables – For the regression tree model, give a plot of the final fit – For the bagged tree and random forest models, produce a variable importance plot ∗ For the random forest model, this is a bit complicated. Check this out and see if you can get it to work.\nMLR Final Model Fit and Summary\n\nMLR_final |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\n# A tibble: 21 × 5\n   term           estimate std.error statistic      p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)      13969.     2527.    5.53   0.0000000811\n 2 SUM_RAIN_FALL    -1906.      651.   -2.93   0.00372     \n 3 SUM_SNOW_FALL     -107.      364.   -0.295  0.768       \n 4 TEMP              -464.     6330.   -0.0733 0.942       \n 5 RH               -2246.     2503.   -0.897  0.370       \n 6 WD_SPD            -635.      374.   -1.70   0.0907      \n 7 VIS                579.      500.    1.16   0.248       \n 8 DP_TEMP           5503.     7645.    0.720  0.472       \n 9 SOL_RAD           2921.      600.    4.87   0.00000197  \n10 SEASONS_Spring    2930.     2560.    1.14   0.253       \n# ℹ 11 more rows\n\n\nLASSO Final Model Fit and Summary\n\nLASSO_final |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\nWarning: package 'glmnet' was built under R version 4.5.2\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-10\n\n\n# A tibble: 14 × 3\n   term             estimate      penalty\n   &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)     13125.    0.0000000001\n 2 SUM_RAIN_FALL   -1647.    0.0000000001\n 3 SUM_SNOW_FALL    -162.    0.0000000001\n 4 TEMP             3003.    0.0000000001\n 5 RH               -508.    0.0000000001\n 6 WD_SPD           -835.    0.0000000001\n 7 VIS               102.    0.0000000001\n 8 DP_TEMP             0.301 0.0000000001\n 9 SOL_RAD          3679.    0.0000000001\n10 SEASONS_Spring   3084.    0.0000000001\n11 SEASONS_Summer   7194.    0.0000000001\n12 SEASONS_Autumn   6410.    0.0000000001\n13 HOLIDAY_Holiday -4513.    0.0000000001\n14 DAY_TYPE_WKEND  -1503.    0.0000000001\n\n\nRTREE Final Model Fit and Summary\n\nRTREE_final |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(cex = 0.5 , roundint = FALSE)\n\n\n\n\n\n\n\n\nBTREE Final Model Fit and Summary\n\nBTREE_final &lt;- BTREE_final |&gt;\n  extract_fit_engine()\n\nBTREE_final$imp |&gt;\n  arrange(value) |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nRF Final Model Fit and Summary\n\nRF_final |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 10)"
  },
  {
    "objectID": "HW9_Modeling_Practice.html#fit-the-final-rf-model-to-the-entire-data-set",
    "href": "HW9_Modeling_Practice.html#fit-the-final-rf-model-to-the-entire-data-set",
    "title": "HW9 Modeling Practice",
    "section": "Fit the Final RF Model to the Entire Data Set",
    "text": "Fit the Final RF Model to the Entire Data Set\n\nFinal_Model &lt;- RF_wkf |&gt;\n  finalize_workflow(RF_best) |&gt;\n  fit(bd_sub) |&gt;\n   extract_fit_parsnip() |&gt;\n  vip(num_features = 10)\n\nFinal_Model"
  }
]